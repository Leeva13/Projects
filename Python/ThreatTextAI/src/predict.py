import torch
from transformers import BertTokenizer, BertForSequenceClassification
import sys
import os

LABELS = ["attack", "movement", "disinfo", "neutral"]

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä–∞ —Ç–∞ –º–æ–¥–µ–ª—ñ
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
if os.path.exists("models/best_model.pth"):
    model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=4)
    model.load_state_dict(torch.load("models/best_model.pth", map_location=torch.device('cpu'), weights_only=True))
else:
    model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=4)  # –ü–æ—á–∞—Ç–∫–æ–≤–∞ –º–æ–¥–µ–ª—å
model.eval()

def predict(text: str):
    encoding = tokenizer(text, padding='max_length', max_length=128, truncation=True, return_tensors='pt')
    input_ids = encoding['input_ids'].to(torch.device('cpu'))
    attention_mask = encoding['attention_mask'].to(torch.device('cpu'))
    
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        pred = torch.argmax(outputs.logits, dim=1).item()
        return LABELS[pred]

if __name__ == '__main__':
    sample_text = " ".join(sys.argv[1:]) if len(sys.argv) > 1 else "–í–æ—Ä–æ–≥ –æ–±—Å—Ç—Ä—ñ–ª—è–≤ –ø—ñ–¥—Å—Ç–∞–Ω—Ü—ñ—é"
    prediction = predict(sample_text)
    print(f"\nüîç –¢–µ–∫—Å—Ç: {sample_text}\nüß† –ö–ª–∞—Å: {prediction}")